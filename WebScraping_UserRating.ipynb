{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e4fa99-c6da-47c0-be7a-77f5cc010901",
   "metadata": {},
   "source": [
    "# Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63a5b394-bb56-4baa-876a-9ca6f49faa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "997ee411-14ab-4d83-b89e-551628084904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping Wine.com\n",
    "base_url = 'https://www.wine.com/list/wine/7155/{}?showOutOfStock=true&sortBy=userRatingDesc'\n",
    "\n",
    "# Function to scrape a single review from wine.com and put data into a pandas series\n",
    "def scrape_winecom_review(review_href):\n",
    "    # Web Scraping Wine.com review\n",
    "    url = 'https://www.wine.com' + review_href\n",
    "    response = requests.Session().get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Defining the data structure\n",
    "    data = {'Name': np.nan,\n",
    "         'Variety': np.nan,\n",
    "         'Origin': np.nan,\n",
    "         'Attr_1': np.nan,\n",
    "         'Attr_2': np.nan,\n",
    "         'Attr_3': np.nan,\n",
    "         'Attr_4': np.nan,\n",
    "         'Alcohol_vol': np.nan,\n",
    "         'Alcohol_percentage': np.nan,\n",
    "         'Winemaker_notes': np.nan,\n",
    "         'Review': np.nan,\n",
    "         'Avg_rating': np.nan,\n",
    "         'N_ratings': np.nan,\n",
    "         'Price_Out-of-stock': np.nan,\n",
    "         'Price': np.nan}\n",
    "    \n",
    "    # Use a list for the case there is multiple records\n",
    "    data_list = [data]\n",
    "\n",
    "    # Name of the wine\n",
    "    try:\n",
    "        name = soup.find_all('h1', {'class':'pipName', 'itemprop':'name'})[0].get_text()\n",
    "        data['Name'] = name\n",
    "    except:\n",
    "        data['Name'] = np.nan\n",
    "\n",
    "    # Variety and origin\n",
    "    try:\n",
    "        var_ori = soup.find_all('a', {'class':'pipOrigin_link'})\n",
    "        for i in range(2):\n",
    "            if i == 0:\n",
    "                variety = var_ori[0].get_text()\n",
    "                data['Variety'] = variety\n",
    "            else:\n",
    "                origin = var_ori[1].get_text()\n",
    "                data['Origin'] = origin\n",
    "    except:\n",
    "        data['Variety'] = np.nan\n",
    "        data['Origin'] = np.nan\n",
    "\n",
    "    # Product Attributes\n",
    "    try:\n",
    "        attr = soup.find_all('ul', {'class':'prodAttr', 'aria-label':'product attributes'})\n",
    "        for i, at in enumerate(attr):\n",
    "            data['Attr_{}'.format(i+1)] = at.find_all('li')[i].get('title')\n",
    "    except:\n",
    "        data['Attr_1'] = np.nan\n",
    "\n",
    "    # Alcohol volume\n",
    "    try:\n",
    "        alcvol = soup.find('span', {'class':'prodAlcoholVolume_text'}).get_text()\n",
    "        data['Alcohol_vol'] = alcvol\n",
    "    except:\n",
    "        data['Alcohol_vol'] = np.nan\n",
    "\n",
    "    # Alcohol Percentage\n",
    "    try:\n",
    "        alcper = soup.find('span', {'class':'prodAlcoholPercent_percent'}).get_text()\n",
    "        data['Alcohol_percentage'] = alcper\n",
    "    except:\n",
    "        data['Alcohol_percentage'] = np.nan\n",
    "\n",
    "    # Winemaker notes\n",
    "    try:\n",
    "        wine_notes = soup.find('div', {'class':'viewMoreModule_text'}).find_all('p')\n",
    "        wine_notes_text = [wn.get_text() for wn in wine_notes]\n",
    "        wine_text = ' '.join(wine_notes_text)\n",
    "        data['Winemaker_notes'] = wine_text\n",
    "    except:\n",
    "        data['Winemaker_notes'] = np.nan\n",
    "\n",
    "    # Rating\n",
    "    try:\n",
    "        rating = soup.find('span', {'class':'averageRating_average', 'itemprop':'ratingValue'}).get_text()\n",
    "        data['Avg_rating'] = rating\n",
    "    except:\n",
    "        data['Avg_rating'] = np.nan\n",
    "\n",
    "    # Number of Ratings\n",
    "    try:\n",
    "        n_ratings = soup.find('span', {'class':'averageRating_number', 'itemprop':'ratingCount'}).get_text()\n",
    "        data['N_ratings'] = n_ratings\n",
    "    except:\n",
    "        data['N_ratings'] = np.nan\n",
    "\n",
    "    # Price out of stock\n",
    "    try:\n",
    "        price_oos = soup.find('span', {'class':'prodItemStock_soldOut-smallText'}).get_text().split(' ')[1].strip('$').strip(')')\n",
    "    except:\n",
    "        price_oos = np.nan\n",
    "    data['Price_Out-of-stock'] = price_oos\n",
    "\n",
    "    try:\n",
    "        price_now = soup.find('span', {'class':'prodItemStock_soldOut-vintagePriceWhole'}).get_text() + '.' + soup.find('span', {'class':'prodItemStock_soldOut-vintagePriceFractional'}).get_text()\n",
    "    except:\n",
    "        price_now = np.nan\n",
    "    data['Price'] = price_now\n",
    "    \n",
    "    # Reviews need to be treated differently, multiple reviews require multiple records in order to keep all the data as possible\n",
    "    try:\n",
    "        reviews = soup.find_all('div', {'class':'pipSecContent_copy'})\n",
    "        reviews_text = [r.get_text() for r in reviews]\n",
    "        for i, rev in enumerate(reviews_text):\n",
    "            if i == 0:\n",
    "                data['Review'] = rev\n",
    "            elif len(reviews_text) > 1:\n",
    "                d = data.copy()\n",
    "                d['Review'] = rev\n",
    "                data_list.append(d)\n",
    "    except:\n",
    "        data['Review'] = np.nan\n",
    "\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def save_data_to_csv(list_dicts, filepath):\n",
    "    # Variables scraped from the website\n",
    "    field_names = ['Name', 'Variety', 'Origin', 'Attr_1', 'Attr_2', 'Attr_3', 'Attr_4', 'Alcohol_vol', 'Alcohol_percentage', 'Winemaker_notes', 'Review',\n",
    "                   'Avg_rating', 'N_ratings', 'Price_Out-of-stock', 'Price']\n",
    "    \n",
    "    # Name of the file to store data\n",
    "    with open(filepath, 'a+', newline='') as csvfile:\n",
    "        # Create a csvwriter object\n",
    "        csvwriter = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "        \n",
    "        # Check if the csv file is empty, if so, write the header\n",
    "        csvfile.seek(0)\n",
    "        first_char = csvfile.read(1)\n",
    "        if not first_char:\n",
    "            csvwriter.writeheader()\n",
    "            \n",
    "        # Write the data\n",
    "        for row in list_dicts:\n",
    "            csvwriter.writerow(row)\n",
    "            \n",
    "        csvfile.close()\n",
    "\n",
    "\n",
    "def get_review_links_winecom(base_url, pages_to_scrape):\n",
    "    links = []\n",
    "    \n",
    "    # Loop that iterates over the quantity of pages to extract review links for\n",
    "    for page_num in range(1, pages_to_scrape+1):\n",
    "        response = requests.Session().get(base_url.format(page_num))\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        lists = soup.find_all('a', {'class':'listGridItemName event_productClick productNoShowPrice'})\n",
    "        hrefs = [l.get('href') for l in lists] # Get href for each wine review\n",
    "        links.extend(hrefs)\n",
    "    return links\n",
    "\n",
    "\n",
    "def scrape_winecom(base_url, pages_to_scrape, filepath):\n",
    "    links = get_review_links_winecom(base_url, pages_to_scrape)\n",
    "    for i, link in enumerate(links):\n",
    "        save_data_to_csv(scrape_winecom_review(link), filepath)\n",
    "        if (i % 1000 == 0):\n",
    "            print(i)\n",
    "    return pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a1b55-18b9-4855-ab93-4857202cfbf5",
   "metadata": {},
   "source": [
    "## GPT Optimized code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604494c2-e3dc-4539-bece-e937cff789c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import numpy as np\n",
    "import concurrent.futures  # For multithreading\n",
    "\n",
    "BASE_URL = 'https://www.wine.com/list/wine/7155/{}?showOutOfStock=true&sortBy=mostInteresting'\n",
    "CSV_FILE = 'wine_data.csv'\n",
    "FIELD_NAMES = ['Name', 'Variety', 'Origin', 'Attr_1', 'Attr_2', 'Attr_3', 'Attr_4',\n",
    "               'Alcohol_vol', 'Alcohol_percentage', 'Winemaker_notes', 'Review',\n",
    "               'Avg_rating', 'N_ratings', 'Price_Out-of-stock', 'Price']\n",
    "\n",
    "# Create a session and reuse it for requests\n",
    "session = requests.Session()\n",
    "\n",
    "def scrape_single_review(review_href):\n",
    "    url = 'https://www.wine.com' + review_href\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except (requests.RequestException, ValueError, AttributeError):\n",
    "        return None  # Handle errors gracefully\n",
    "\n",
    "    data = {field: np.nan for field in FIELD_NAMES}\n",
    "\n",
    "    try:\n",
    "        data['Name'] = soup.find('h1', {'class':'pipName', 'itemprop':'name'}).get_text()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    # Other data extraction code...\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_data_to_csv(data_list):\n",
    "    with open(CSV_FILE, 'a+', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.DictWriter(csvfile, fieldnames=FIELD_NAMES)\n",
    "        csvfile.seek(0)\n",
    "        first_char = csvfile.read(1)\n",
    "        if not first_char:\n",
    "            csvwriter.writeheader()\n",
    "        \n",
    "        csvwriter.writerows(data_list)\n",
    "\n",
    "def scrape_winecom_page(page_num):\n",
    "    links = get_review_links_winecom(BASE_URL, page_num)\n",
    "    data_list = []\n",
    "\n",
    "    for link in links:\n",
    "        data = scrape_single_review(link)\n",
    "        if data:\n",
    "            data_list.append(data)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def get_review_links_winecom(base_url, page_num):\n",
    "    response = session.get(base_url.format(page_num))\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    lists = soup.find_all('a', {'class':'listGridItemName event_productClick productNoShowPrice'})\n",
    "    hrefs = [l.get('href') for l in lists]\n",
    "    return hrefs\n",
    "\n",
    "def main():\n",
    "    pages_to_scrape = 1000  # Adjust the number of pages as needed\n",
    "    concurrent_requests = 10  # Adjust the number of concurrent requests\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_requests) as executor:\n",
    "        # Scrape data concurrently from multiple pages\n",
    "        all_data = executor.map(scrape_winecom_page, range(1, pages_to_scrape + 1))\n",
    "\n",
    "    # Flatten the list of lists returned by executor.map\n",
    "    flattened_data = [data for page_data in all_data for data in page_data]\n",
    "\n",
    "    # Save data to CSV\n",
    "    save_data_to_csv(flattened_data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466719ed-31a6-4fa6-a7a8-d42078e1b447",
   "metadata": {},
   "source": [
    "## GPT Parallel Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc81c94-d415-4b00-86c8-7c002190b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# Function to scrape a single review from wine.com and put data into a dictionary\n",
    "def scrape_winecom_review(review_href):\n",
    "    url = 'https://www.wine.com' + review_href\n",
    "    response = session.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Predefine a data structure to not have anomalies in each record\n",
    "    data = {\n",
    "        'Name': np.nan,\n",
    "        'Variety': np.nan,\n",
    "        'Origin': np.nan,\n",
    "        'Attr_1': np.nan,\n",
    "        'Attr_2': np.nan,\n",
    "        'Alcohol_vol': np.nan,\n",
    "        'Alcohol_percentage': np.nan,\n",
    "        'Winemaker_notes': np.nan,\n",
    "        'Review': np.nan,\n",
    "        'Avg_rating': np.nan,\n",
    "        'N_ratings': np.nan,\n",
    "        'Price_Out-of-stock': np.nan,\n",
    "        'Price': np.nan\n",
    "    }\n",
    "    \n",
    "    # Get wine name\n",
    "    try:\n",
    "        name = soup.find('h1', {'class': 'pipName', 'itemprop': 'name'}).get_text()\n",
    "        data['Name'] = name\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Get wine variety and origin\n",
    "    try:\n",
    "        var_ori = soup.find_all('a', {'class': 'pipOrigin_link'})\n",
    "        data['Variety'] = var_ori[0].get_text()\n",
    "        data['Origin'] = var_ori[1].get_text()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Get attributes, max of 4 attributes for each wine\n",
    "    try:\n",
    "        attr = soup.find_all('ul', {'class': 'prodAttr', 'aria-label': 'product attributes'})\n",
    "        for i, at in enumerate(attr):\n",
    "            data[f'Attr_{i+1}'] = at.find_all('li')[i].get('title')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # get the alcohol volume of the wine\n",
    "    try:\n",
    "        data['Alcohol_vol'] = soup.find('span', {'class': 'prodAlcoholVolume_text'}).get_text()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Get the alcohol percentage of the wine\n",
    "    try:\n",
    "        data['Alcohol_percentage'] = soup.find('span', {'class': 'prodAlcoholPercent_percent'}).get_text()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Get winemaker notes\n",
    "    try:\n",
    "        wine_notes = soup.find('div', {'class': 'viewMoreModule_text'}).find_all('p')\n",
    "        data['Winemaker_notes'] = ' '.join(wn.get_text() for wn in wine_notes)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Get average rating for the wine\n",
    "    try:\n",
    "        data['Avg_rating'] = soup.find('span', {'class': 'averageRating_average', 'itemprop': 'ratingValue'}).get_text()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Get the number of rating for that wine\n",
    "    try:\n",
    "        data['N_ratings'] = soup.find('span', {'class': 'averageRating_number', 'itemprop': 'ratingCount'}).get_text()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Get price out of stock for the wine\n",
    "    try:\n",
    "        price_oos = soup.find('span', {'class': 'prodItemStock_soldOut-smallText'}).get_text().split(' ')[1].strip('$').strip(')')\n",
    "        data['Price_Out-of-stock'] = price_oos\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Get the actual price of the wine when available\n",
    "    try:\n",
    "        price_now = soup.find('span', {'class': 'prodItemStock_soldOut-vintagePriceWhole'}).get_text() + '.' + soup.find('span', {'class': 'prodItemStock_soldOut-vintagePriceFractional'}).get_text()\n",
    "        data['Price'] = price_now\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Get the most important review for the wine\n",
    "    try:\n",
    "        reviews = soup.find('div', {'class': 'pipSecContent_copy'})\n",
    "        data['Review'] = reviews.get_text()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Return the data dictionary\n",
    "    return data\n",
    "\n",
    "# Function to save a batch of data to CSV\n",
    "def save_data_to_csv(batch, csv_filepath):\n",
    "    # Pre-defined data structure\n",
    "    field_names = ['Name', 'Variety', 'Origin', 'Attr_1', 'Attr_2', 'Alcohol_vol', 'Alcohol_percentage', \n",
    "                   'Winemaker_notes', 'Review', 'Avg_rating', 'N_ratings', 'Price_Out-of-stock', 'Price']\n",
    "\n",
    "    # Open a csv file in append+write mode with utf-8 encoding\n",
    "    with open(csv_filepath, 'a+', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.DictWriter(csvfile, fieldnames=field_names) # Specify the columns to be written\n",
    "\n",
    "        # Check if the CSV file is empty, if so, write the header\n",
    "        csvfile.seek(0)\n",
    "        first_char = csvfile.read(1)\n",
    "        if not first_char:\n",
    "            csvwriter.writeheader()\n",
    "\n",
    "        # Write the batch of data\n",
    "        for data_dict in batch:\n",
    "            csvwriter.writerow(data_dict)\n",
    "\n",
    "        csvfile.close()\n",
    "\n",
    "# Function to scrape wine.com reviews using multiprocessing\n",
    "def scrape_winecom_parallel(base_url, csv_filepath, txt_filepath, get_links=True, pages_to_scrape=(1,1), num_threads=4, batch_size=1000):\n",
    "    start_time = time.time() # Get start time\n",
    "    \n",
    "    # Execute function to get all the links of the desired pages\n",
    "    if get_links:\n",
    "        get_review_links_winecom(base_url, txt_filepath, pages_to_scrape=pages_to_scrape)\n",
    "    \n",
    "    # Get all links\n",
    "    links = read_review_links_from_file(txt_filepath)\n",
    "    \n",
    "    end_time = time.time() # Get finished time\n",
    "    elapsed_time = end_time - start_time # Get total time\n",
    "    print('Time to gather all the links for {} scraped pages:'.format(pages_to_scrape), elapsed_time)\n",
    "    print('Number of links recollected:', len(links))\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        data_batches = []\n",
    "        current_batch = []\n",
    "        batch_count = 0\n",
    "\n",
    "        for i, link in enumerate(links):\n",
    "            current_batch.append(link)\n",
    "\n",
    "            # When the batch size is reached or we have processed all links, scrape and save the batch\n",
    "            if len(current_batch) == batch_size or i == len(links) - 1:\n",
    "                batch_count += 1\n",
    "                \n",
    "                if batch_count % 1000 == 0:\n",
    "                    start_time = time.time()  # Record the start time before processing 1000 batches\n",
    "            \n",
    "                data_batch = executor.map(scrape_winecom_review, current_batch)\n",
    "                data_batches.extend(data_batch)\n",
    "                current_batch = []\n",
    "                \n",
    "                # If we've processed 1000 batches, measure and print the elapsed time\n",
    "                if batch_count % 1000 == 0:\n",
    "                    end_time = time.time()  # Record the end time after processing 1000 batches\n",
    "                    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "                    print(f\"Processed 1000 batches in {elapsed_time:.2f} seconds\")  # Print the elapsed time\n",
    "\n",
    "            if (i % 1000 == 0):\n",
    "                print(i)\n",
    "\n",
    "        # Save all the data in batches\n",
    "        while data_batches:\n",
    "            batch_to_save = data_batches[:batch_size]\n",
    "            save_data_to_csv(batch_to_save, csv_filepath)\n",
    "            data_batches = data_batches[batch_size:]\n",
    "\n",
    "    return pd.read_csv(csv_filepath, encoding='utf-8')\n",
    "\n",
    "# Function to get review links from wine.com\n",
    "def get_review_links_winecom(base_url, txt_filepath, pages_to_scrape=(1,1)):\n",
    "    # Open the text file in write mode\n",
    "    with open(txt_filepath, 'w') as link_file:\n",
    "        for page_num in range(pages_to_scrape[0], pages_to_scrape[1] + 1):\n",
    "            response = session.get(base_url.format(page_num))\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            lists = soup.find_all('a', {'class': 'listGridItemName event_productClick productNoShowPrice'})\n",
    "            hrefs = [l.get('href') for l in lists]  # Get href for each wine review\n",
    "            \n",
    "            # Write the links to the text file\n",
    "            for href in hrefs:\n",
    "                link_file.write(href + '\\n')\n",
    "                \n",
    "# Function to read review links from a text file\n",
    "def read_review_links_from_file(txt_filepath):\n",
    "    links = []\n",
    "\n",
    "    with open(txt_filepath, 'r') as link_file:\n",
    "        for line in link_file:\n",
    "            link = line.strip()  # Remove leading/trailing whitespace and newline characters\n",
    "            links.append(link)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dbc4220-3038-4490-a569-38608345a0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to gather all the links for (1, 18000) scraped pages: 39488.89217042923\n",
      "Number of links recollected: 432660\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n"
     ]
    },
    {
     "ename": "TooManyRedirects",
     "evalue": "Exceeded 30 redirects.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTooManyRedirects\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\GitProjects\\DataSommeliers\\WebScraping_UserRating.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m session \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mSession()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Create new DataFrame with data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df \u001b[39m=\u001b[39m scrape_winecom_parallel(base_url, csv_filepath, txt_filepath, get_links\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, pages_to_scrape\u001b[39m=\u001b[39mpages_to_scrape,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                             num_threads\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m df\n",
      "\u001b[1;32mc:\\GitProjects\\DataSommeliers\\WebScraping_UserRating.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()  \u001b[39m# Record the start time before processing 1000 batches\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m data_batch \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39mmap(scrape_winecom_review, current_batch)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m data_batches\u001b[39m.\u001b[39mextend(data_batch)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m current_batch \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m \u001b[39m# If we've processed 1000 batches, measure and print the elapsed time\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\46708\\anaconda3\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop())\n\u001b[0;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mc:\\Users\\46708\\anaconda3\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39mresult(timeout)\n\u001b[0;32m    318\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[1;32mc:\\Users\\46708\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\46708\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\46708\\anaconda3\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "\u001b[1;32mc:\\GitProjects\\DataSommeliers\\WebScraping_UserRating.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscrape_winecom_review\u001b[39m(review_href):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://www.wine.com\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m review_href\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39mget(url)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitProjects/DataSommeliers/WebScraping_UserRating.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Predefine a data structure to not have anomalies in each record\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\46708\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[39m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(\u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\46708\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\46708\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:725\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    723\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    724\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 725\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[0;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    727\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\46708\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:725\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    723\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    724\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 725\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[0;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    727\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\46708\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:191\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m     resp\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mread(decode_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    190\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(resp\u001b[39m.\u001b[39mhistory) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_redirects:\n\u001b[1;32m--> 191\u001b[0m     \u001b[39mraise\u001b[39;00m TooManyRedirects(\n\u001b[0;32m    192\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExceeded \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_redirects\u001b[39m}\u001b[39;00m\u001b[39m redirects.\u001b[39m\u001b[39m\"\u001b[39m, response\u001b[39m=\u001b[39mresp\n\u001b[0;32m    193\u001b[0m     )\n\u001b[0;32m    195\u001b[0m \u001b[39m# Release the connection back into the pool.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m resp\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;31mTooManyRedirects\u001b[0m: Exceeded 30 redirects."
     ]
    }
   ],
   "source": [
    "# Attempt\n",
    "base_url = 'https://www.wine.com/list/wine/7155/{}?showOutOfStock=true&sortBy=userRatingDesc'\n",
    "pages_to_scrape = (1,18_000)  # Adjust as needed\n",
    "csv_filepath = 'wine_reviews_user.csv'\n",
    "txt_filepath = 'reviews_user.txt'\n",
    "\n",
    "# Create a session for making requests\n",
    "session = requests.Session()\n",
    "\n",
    "# Create new DataFrame with data\n",
    "df = scrape_winecom_parallel(base_url, csv_filepath, txt_filepath, get_links=True, pages_to_scrape=pages_to_scrape,\n",
    "                            num_threads=15, batch_size=1000)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
